
<<load metadata>>=

#LOAD REQUIRED PACKAGES
require(data.table)
require(plyr)
require(dplyr)

options(scipen=10)
#data_directory<-"/Users/polly/Desktop/projects/heterochromatin/great_ape_diversity/counts/"
data_directory<-"/Users/polly/Desktop/projects/heterochromatin/refactored/" #"/Users/polly/Desktop/projects/heterochromatin/great_ape_diversity/counts75/"
working_directory<-"/Users/polly/Desktop/projects/heterochromatin/refactored/"
setwd(working_directory)

#CHECK IF FILES EXIST
checkExistenceOfFile <- function(file) {
  if (!file.exists(file)) {
    warning(paste("File",file,"does not exist. ERROR."))
  }
}

#list of files that we need to check. Print warning message if non-existent.
invisible(lapply(c("rcounts","layout","contaminated_individuals","related_individuals","rlengths"),checkExistenceOfFile))

#LOAD READ COUNTS
#load read counts after filtering for great apes
readC_afterF = read.table("rcounts",sep=",",header=TRUE) #SRR id followed by read counts after filtering

#LOAD METADATA ABOUT SAMPLES
#Run spots bases LibraryLayout ScientificName SampleName Sex
layout = read.table("layout", header=TRUE, sep = ",")
layout<-layout[layout$Run %in% readC_afterF$Run,] #keep only those that have successful filtering information

#PRINT NUMBER OF DATASETS (NOT INDIVIDUALS) for each species
#print(table(layout$ScientificName,layout$Sex))

#PRINT SIZE OF DATASET FOR EACH INDIVIDUAL
#print(aggregate(layout$bases ~ layout$SampleName + layout$Sex, layout$Sex, FUN=mean))

getFileNames <- function(sex=NULL,species=NULL,name=NULL){
  #print(sex) 
  #print(species) 
  #print(name)
  list <-layout
  if (!is.null(sex)) {list <-list[list$Sex==sex,]}
  if (!is.null(species)) {list <-list[list$ScientificName==species,]}
  if (!is.null(name)) {list <-list[list$SampleName==name,]}
  filenames<-paste(list$Run,gsub(" ","_", list$ScientificName),gsub(" ","_", list$SampleName),list$Sex,"1.fastq",sep="|")
  #print(list)
  return(filenames) #includes only R1 files
}

getFilteredReadCounts <- function(files) {
  SRR_ids<-gsub( "\\|.*$", "", files)
  merged<-merge(SRR_ids,readC_afterF,by=1)
  read_counts<-merged[match(SRR_ids,merged$x),]$rCount
  
  if (!length(SRR_ids)==length(read_counts)) {
    stop("Discrepancies in the association table.")
  }
  
  if (any(is.na(read_counts))) {
    stop("Discrepancies in the read length association.")
  }
  
  return(read_counts)
}

getReadLengths <- function(files) {
  SRR_ids<-gsub( "\\|.*$", "", files)
  merged<-merge(SRR_ids,read_lengths,by=1)
  rl<-merged[match(SRR_ids,merged$x),]$read_length
  
  if (!length(SRR_ids)==length(rl)) {
    stop("Discrepancies in the association table.")
  }
  
  if (any(is.na(rl))) {
    stop("Discrepancies in the read length association.")
  }
  
  return(rl)
}


#filters = list(species="Pan paniscus")
filters = list() #no filters here, however filters could be potentially specified

#GET LIST OF ALL DATASETS AVAILABLE
files<-do.call(getFileNames, filters)

# get the size for each file
#sizes <- file.info(list.files(data_directory,full.names=TRUE))

# subset the files that have non-zero size
#list.of.non.empty.files <- basename(rownames(sizes[sizes$size>0,]))

#files<-files[gsub("\\|","_",files) %in% list.of.non.empty.files] #keep only non-empty files
@


<<load data from previous section, keep only those datasets/individuals that pass filters>>=

dropLowReadCounts <-function(files,read_count_threshold) {
  if (!length(getFilteredReadCounts(files))==length(files)) {
    stop("Discrepancies in the read length association.")
  }
  keep<-getFilteredReadCounts(files)>read_count_threshold
  print(paste("excluding this many files:",as.numeric(table(keep)["FALSE"])))
  print(paste("returning this many files:",as.numeric(table(keep)["TRUE"])))
  return(files[keep])
}

dropShortReads <-function(files,read_length_threshold) { 
  SRR_ids<-gsub("\\|.*$", "",files)
  merged<-merge(SRR_ids,read_lengths,by=1)
  colnames(merged)<-c("file","read_length")
  read_lens<-merged[match(SRR_ids,merged$file),]$read_length
  
  if (!length(files)==length(read_lens)) {
    stop("Discrepancies in the read length association.")
  }
  
  if (any(is.na(read_lens))) {
    stop("Discrepancies in the read length association.")
  }
  
  print(read_length_threshold)
  SRR_ids_to_exclude<-read_lengths[read_lengths$read_length<read_length_threshold,]$file
  SRR_ids_to_keep<-read_lengths[read_lengths$read_length>=read_length_threshold,]$file
  
  print(paste("excluding this many files:",length(SRR_ids_to_exclude)))
  if (length(SRR_ids_to_exclude)==0) {
      print(paste("returning this many files:",length(files)))
      return(files)
  } else {
      keep<-gsub("\\|.*","",files) %in% SRR_ids_to_keep
      print(paste("returning this many files:",length(files[keep])))
      return(files[keep])
  }
}

dropContaminatedFiles <-function(files) {
  keep<-!grepl(paste(unlist(contaminated),collapse="|"),files)
  print(paste("excluding this many files:",as.numeric(table(keep)["FALSE"])))
  print(paste("returning this many files:",as.numeric(table(keep)["TRUE"])))
  return(files[keep])
}

dropFamilyRelatedFiles <-function(files) {
  keep<-!grepl(paste(unlist(related),collapse="|"),files)
  print(paste("excluding this many files:",as.numeric(table(keep)["FALSE"])))
  print(paste("returning this many files:",as.numeric(table(keep)["TRUE"])))
  return(files[keep])
}

contaminated = read.table("contaminated_individuals", header=FALSE)
related = read.table("related_individuals", header=FALSE)
read_lengths = as.data.frame(read.table("rlengths", sep=",",header=TRUE))
read_lengths<-read_lengths[grep("1.fastq",read_lengths$file),] #keep only forward reads
read_lengths$file<-gsub("_.*","",read_lengths$file)
read_lengths$file<-gsub('[.].*',"",read_lengths$file)

filtered_files<-files

#drop files with too short reads
filtered_files<-dropShortReads(filtered_files,52) #exclude everything shorter than a given threshold

#drop files with very low read counts
filtered_files<-dropLowReadCounts(filtered_files,20000000) #threshold representing minimal number of reads for sample to be taken into consideration

#drop contaminated individuals
#filtered_files<-dropContaminatedFiles(filtered_files)

#drop related individuals that could bias heterochromatin abundance counts
filtered_files<-dropFamilyRelatedFiles(filtered_files)
@

<<FREQUENCY>>=
#d<-read.table("/Users/polly/Desktop/projects/heterochromatin/great_ape_diversity/counts75/threshold100/sort/big.table.with.header.txt",sep=" ",header=TRUE,row.names=1,na.strings=NA,fill=TRUE)
  
d<-read.table("/Users/polly/Desktop/projects/heterochromatin/refactored/big.table.with.header.rawcounts.sortedFilt.txt",sep=" ",header=TRUE,row.names=1,na.strings=NA,fill=TRUE,check.names=FALSE)

extra_human_d<-read.table("/Users/polly/Desktop/projects/heterochromatin/refactored/human.table.with.header.rawcounts.sortedFilt.txt",sep=" ",header=TRUE,row.names=1,na.strings=NA,fill=TRUE,check.names=FALSE)

#d<-as.data.frame(merge(d,extra_human_d,by="row.names",all.x=TRUE)) #convert to dataframe
d<-merge(d,extra_human_d,by="row.names",all.x=TRUE)
print(tail(colnames(d),3))
rownames(d)<-d[,"Row.names"]
d<-d[,-which(names(d) == "Row.names")] 
d<-as.data.frame(d) #convert to dataframe
d[is.na(d)] <- 0

columns_to_select<-paste0(gsub( "\\|", "_",filtered_files),".dat_Header.txt.rawcounts.sortedFilt")
data<-d[,columns_to_select] 
data<-data[order(-rowSums(data)),]
print(dim(data))

rc_by_sample<-readC_afterF
rc_by_sample$minCount<-(rc_by_sample$rCount/20000000*100)

idstobekept<-gsub("\\|.*","",filtered_files)
dim(rc_by_sample[rc_by_sample$Run %in% idstobekept,])
rc_by_sample<-rc_by_sample[rc_by_sample$Run %in% idstobekept,]

sample_names<-as.data.frame(gsub("\\_.*","",colnames(data)))
colnames(sample_names)<-c("Run")
mincount_df<-merge(sample_names,rc_by_sample,sort=FALSE)
print(head(mincount_df))

stopifnot(identical(as.vector(unlist(mincount_df$Run)),as.vector(unlist(sample_names))))

set_threshold_sample<-function(repeat_row) {
  stopifnot(nrow(repeat_row)==1)
  repeat_row[repeat_row<mincount_df$minCount]<-NA
  return(repeat_row)
}
#set_threshold_sample(data[5,]) #test on single repeated motif 
#View(t(data[10:20,]))
#View((apply(data[10:20,],1,set_threshold_sample)))

filtered_data<-t(apply(data,1,set_threshold_sample)) #todo: why do I need transposition?
print(dim(filtered_data))
@

<<frequency_continuation>>=
rep.row<-function(x,n){
   matrix(rep(x,each=n),nrow=n) #transforms normalization vector into matrix
}

#NORMALIZE BY READ COUNTS per million reads
normalization_factor=1000000
normalization_matrix<-rep.row(getFilteredReadCounts(filtered_files),dim(filtered_data)[1])
filtered_data<-as.data.frame(filtered_data) #convert to dataframe from data table
filtered_data<-filtered_data/normalization_matrix*normalization_factor #normalization of data by read counts

#NORMALIZE BY READ LENGTH per 100bp reads
normalization_factor=100
normalization_matrix<-rep.row(getReadLengths(filtered_files),dim(filtered_data)[1])
filtered_data<-as.data.frame(filtered_data) #convert to dataframe from data table
filtered_data<-filtered_data/normalization_matrix*normalization_factor #normalization of data by read counts

#FREQUENCY
#here keep only repeated motifs that have minimal number of loci required
frequency<-filtered_data[rowSums(is.na(filtered_data))!=ncol(filtered_data),] #normalized frequency
frequency[is.na(frequency)] <- 0 #set missing values as 0s => repeated motif not detected

frequency<-frequency[order(rowSums(frequency),decreasing=T),]
frequency<-frequency[nchar(as.character(rownames(frequency)))<=50,] #remove handful of repeat motifs with unit sizes over 50
frequency<-as.data.frame(frequency)

plot(table(nchar(as.character(rownames(frequency)))),ylab="number of distinct repeat motifs",xlab="unit size")
@

<<DENSITY>>=
d<-as.data.frame(read.table("/Users/polly/Desktop/projects/heterochromatin/refactored/big.table.with.header.rawlengths.sortedFilt.txt",sep=" ",header=TRUE,row.names=1,na.strings=NA,fill=TRUE,check.names=FALSE))
extra_human_d<-as.data.frame(read.table("human.table.with.header.rawlengths.sortedFilt.txt",sep=" ",header=TRUE,row.names=1,na.strings=NA,fill=TRUE,check.names=FALSE))

#d<-as.data.frame(merge(d,extra_human_d,by="row.names",all.x=TRUE)) #convert to dataframe
d<-merge(d,extra_human_d,by="row.names",all.x=TRUE)

rownames(d)<-d[,"Row.names"]
d<-d[,-which(names(d) == "Row.names")] 
d<-as.data.frame(d) #convert to dataframe

d[is.na(d)] <- 0

columns_to_select<-paste0(gsub( "\\|", "_",filtered_files),".dat_Header.txt.rawlengths.sortedFilt")
data<-d[,columns_to_select] 
data<-data[order(-rowSums(data)),] 
#View(d)

rep.row<-function(x,n){
   matrix(rep(x,each=n),nrow=n) #transforms normalization vector into matrix
}

#NORMALIZE BY READ COUNTS
normalization_factor=1000000
normalization_matrix<-rep.row(getFilteredReadCounts(filtered_files),dim(data)[1])
data<-as.data.frame(data) #convert to dataframe from data table
data<-data/normalization_matrix*normalization_factor #normalization of data by read counts

#NORMALIZE BY READ LENGTH
normalization_factor=100
normalization_matrix<-rep.row(getReadLengths(filtered_files),dim(data)[1])
data<-as.data.frame(data) #convert to dataframe from data table
data<-data/normalization_matrix*normalization_factor #normalization of data by read counts

#DENSITY
density<-data[rownames(frequency),columns_to_select] #restrict only to filtered files and match filtering of frequency variable

plot(table(nchar(as.character(rownames(density)))),ylab="number of distinct repeat motifs",xlab="unit size")
@

<<stat>>=
print(dim(frequency))
print(dim(density))
stopifnot(dim(frequency)==dim(density))

mean(as.numeric(filtered_data["AATGG",]),na.RM=TRUE)
mean(as.numeric(frequency["AATGG",]),na.RM=TRUE)
mean(as.numeric(density["AATGG",]),na.RM=TRUE)
@

<<>>=
geterrmessage()
@

