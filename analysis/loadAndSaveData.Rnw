
<<load metadata>>=

#LOAD REQUIRED PACKAGES
require(data.table)
require(plyr)
require(dplyr)

options(scipen=10)
#data_directory<-"/Users/alice/Desktop/projects/heterochromatin/great_ape_diversity/counts/"
data_directory<-"/Users/alice/Desktop/projects/heterochromatin/great_ape_diversity/counts75/"
working_directory<-"/Users/alice/Desktop/projects/heterochromatin/refactored/"
setwd(working_directory)

#CHECK IF FILES EXIST
checkExistenceOfFile <- function(file) {
  if (!file.exists(file)) {
    warning(paste("File",file,"does not exist. ERROR."))
  }
}

#list of files that we need to check. Print warning message if non-existent.
invisible(lapply(c("readC_afterF","readC_afterF_human","full_layout","human_layout","contaminated_individuals","related_individuals","read_lengths","read_lengths_human"),checkExistenceOfFile))

#LOAD READ COUNTS
#load read counts after filtering for great apes
readC_afterF = read.table(("readC_afterF"),header=FALSE) #SRR id followed by read counts after filtering
#load read counts after filtering for human
readC_afterF_human = read.table(("readC_afterF_human"),header=FALSE)
#concatenate counts of great apes with human data
readC_afterF <- rbind(readC_afterF,readC_afterF_human)

#LOAD METADATA ABOUT SAMPLES
#Run spots bases LibraryLayout ScientificName SampleName Sex
layout = read.table("full_layout", header=TRUE, sep = ",")
layout<-layout[layout$Run %in% readC_afterF$V1,] #keep only those that have successful filtering information



#Run LibraryLayout ScientificName SampleName Sex
human_layout = read.table("human_layout", header=TRUE, sep = ",")
#merge metadata about great apes with metadata about human
layout<-merge(layout,human_layout,all=TRUE)

#PRINT NUMBER OF DATASETS (NOT INDIVIDUALS) for each species
#print(table(layout$ScientificName,layout$Sex))

#PRINT SIZE OF DATASET FOR EACH INDIVIDUAL
#print(aggregate(layout$bases ~ layout$SampleName + layout$Sex, layout$Sex, FUN=mean))

getFileNames <- function(sex=NULL,species=NULL,name=NULL){
  #print(sex) 
  #print(species) 
  #print(name)
  list <-layout
  if (!is.null(sex)) {list <-list[list$Sex==sex,]}
  if (!is.null(species)) {list <-list[list$ScientificName==species,]}
  if (!is.null(name)) {list <-list[list$SampleName==name,]}
  filenames<-paste(list$Run,gsub(" ","_", list$ScientificName),gsub(" ","_", list$SampleName),list$Sex,"1.fastq.counts.sortedFilt",sep="|")
  #print(list)
  return(filenames) #includes only R1 files
}

getFilteredReadCounts <- function(files) {
  SRR_ids<-gsub( "\\|.*$", "", files)
  merged<-merge(SRR_ids,readC_afterF,by=1)
  read_counts<-merged[match(SRR_ids,merged$x),]$V2
  
  if (!length(SRR_ids)==length(read_counts)) {
    stop("Discrepancies in the association table.")
  }
  
  return(read_counts)
}


#filters = list(species="Pan paniscus")
filters = list() #no filters here, however filters could be potentially specified

#GET LIST OF ALL DATASETS AVAILABLE
files<-do.call(getFileNames, filters)

# get the size for each file
sizes <- file.info(list.files(data_directory,full.names=TRUE))

# subset the files that have non-zero size
list.of.non.empty.files <- basename(rownames(sizes[sizes$size>0,]))

files<-files[gsub("\\|","_",files) %in% list.of.non.empty.files] #keep only non-empty files
@


<<load data from previous section, keep only those datasets/individuals that pass filters>>=

dropLowReadCounts <-function(files,read_count_threshold) {
  if (!length(getFilteredReadCounts(files))==length(files)) {
    stop("Discrepancies in the read length association.")
  }
  
  keep<-getFilteredReadCounts(files)>read_count_threshold
  print(paste("excluding this many files:",as.numeric(table(keep)["FALSE"])))
  print(paste("returning this many files:",as.numeric(table(keep)["TRUE"])))
  return(files[keep])
  
  
}

dropShortReads <-function(files,read_length_threshold) { 
  SRR_ids<-gsub( "\\|", "_", gsub(".fastq.*",".fastq",files))
  merged<-merge(SRR_ids,read_lengths,by=1)
  colnames(merged)<-c("file","read_length")
  read_lens<-merged[match(SRR_ids,merged$file),]$read_length
  
  if (!length(files)==length(read_lens)) {
    stop("Discrepancies in the read length association.")
  }
  
  if (any(is.na(read_lens))) {
    stop("Discrepancies in the read length association.")
  }
  
  print(read_length_threshold)
  SRR_ids_to_exclude<-read_lengths[read_lengths$read_length<read_length_threshold,]$file
  SRR_ids_to_keep<-read_lengths[read_lengths$read_length>=read_length_threshold,]$file
  
  print(paste("excluding this many files:",length(SRR_ids_to_exclude)))
  if (length(SRR_ids_to_exclude)==0) {
      print(paste("returning this many files:",length(files)))
      return(files)
  } else {
      keep_prep<-paste0(SRR_ids_to_keep,".counts.sortedFilt")
      keep<-gsub("\\|","_",files) %in% keep_prep
      print(paste("returning this many files:",length(files[keep])))
      return(files[keep])
  }
}

dropContaminatedFiles <-function(files) {
  keep<-!grepl(paste(unlist(contaminated),collapse="|"),files)
  print(paste("excluding this many files:",as.numeric(table(keep)["FALSE"])))
  print(paste("returning this many files:",as.numeric(table(keep)["TRUE"])))
  return(files[keep])
}

dropFamilyRelatedFiles <-function(files) {
  keep<-!grepl(paste(unlist(related),collapse="|"),files)
  print(paste("excluding this many files:",as.numeric(table(keep)["FALSE"])))
  print(paste("returning this many files:",as.numeric(table(keep)["TRUE"])))
  return(files[keep])
}

contaminated = read.table("contaminated_individuals", header=FALSE)
related = read.table("related_individuals", header=FALSE)
read_lengths = read.table("read_lengths", header=FALSE)
read_lengths<-read_lengths[grep("_1.fastq",read_lengths$V1),] #keep only forward reads

#merge with human samples
read_lengths_human = read.table("read_lengths_human", header=FALSE)
read_lengths<-rbind(read_lengths,read_lengths_human)
read_lengths<-as.data.frame(read_lengths)
colnames(read_lengths)<-c("file","read_length")

filtered_files<-files

#drop files with too short reads
filtered_files<-dropShortReads(filtered_files,52) #exclude everything shorter than a given threshold

#drop files with very low read counts
filtered_files<-dropLowReadCounts(filtered_files,20000000) #threshold representing minimal number of reads for sample to be taken into consideration

#drop contaminated individuals
#filtered_files<-dropContaminatedFiles(filtered_files)

#drop related individuals that could bias heterochromatin abundance counts
filtered_files<-dropFamilyRelatedFiles(filtered_files)
@

<<BASH LOADED TABLE NORMALIZATION AND RESTRICTION TO FILTERED FILES>>=
d<-read.table("/Users/alice/Desktop/projects/heterochromatin/great_ape_diversity/counts75/threshold100/sort/big.table.with.header.txt",sep=" ",header=TRUE,row.names=1,na.strings=NA,fill=TRUE)
d<-as.data.frame(d) #convert to dataframe
d[is.na(d)] <- 0

columns_to_select<-gsub( "\\|", "_",filtered_files)
data<-d[order(-rowSums(d)),columns_to_select] 
#View(d)

rep.row<-function(x,n){
   matrix(rep(x,each=n),nrow=n) #transforms normalization vector into matrix
}

normalization_factor=1000000
normalization_matrix<-rep.row(getFilteredReadCounts(filtered_files),dim(data)[1])
data<-as.data.frame(data) #convert to dataframe from data table
data[,2:ncol(data)]<-data[2:ncol(data)]/normalization_matrix*normalization_factor #normalization of data by read counts

abundant<-data[,columns_to_select] #RESTRICT ONLY TO FILTERED FILES
abundant<-abundant[order(rowSums(abundant),decreasing=T),]
threshold<-15 #the cummulative repeat count per million reads needs to be at least 5
abundant<-abundant[rowSums(abundant)>(threshold),] #keep only rows where all three libraries have at least threshold abundance total

abundant<-abundant[nchar(as.character(rownames(abundant)))<=50,] #remove handful of repeat motifs with unit sizes over 50
abundant<-as.data.frame(abundant)

vector_of_unit_lengths<-nchar(as.character(rownames(abundant)))
abundant<-sweep(abundant,MARGIN=1,STATS=vector_of_unit_lengths,FUN="*")  #NOW CALCULATE REPEAT DENSITYL multiply by unit length
abundant<-abundant[order(rowSums(abundant),decreasing=T),]
View(abundant)
print(dim(abundant))
plot(table(nchar(as.character(rownames(abundant)))),ylab="number of distinct repeat motifs",xlab="unit size")
@

