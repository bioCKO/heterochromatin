\documentclass{article}

\begin{document}
\SweaveOpts{concordance=TRUE}

<<Ashkenazim>>=
require(data.table)
require(plyr)
require(dplyr)
options(scipen=10)
rep.row<-function(x,n){
   matrix(rep(x,each=n),nrow=n) #from https://ctszkin.com/2011/09/02/a-quick-way-to-do-rep-row-and-rep-col/
}

read_and_normalize <-function(input_file) {
  input<-read.table(input_file,col.names=c("unit","count"))
  #input$count<-input$count/sum(input$count)*1000000
  #input$count<-input$count<-log(input$count+0.01)
  name<-basename(input_file)
  colnames(input)<-c("unit",name)
  return(as.data.frame(input))
}

setwd("/Volumes/disk/projects_backup/projects/heterochromatin/human_heterochromatin/densities")
motherAshkenazim <- read_and_normalize("HG004_Homo_sapiens_Mother_F_1.fastq.dat_Header.txt.rawlengths.sortedFilt")
fatherAshkenazim <- read_and_normalize("HG003_Homo_sapiens_Father_M_1.fastq.dat_Header.txt.rawlengths.sortedFilt")
sonAshkenazim <- read_and_normalize("HG002_Homo_sapiens_Son_M_1.fastq.dat_Header.txt.rawlengths.sortedFilt")
mother1463 <- read_and_normalize("NA12890_Homo_sapiens_Mother2_F_1.fastq.dat_Header.txt.rawlengths.sortedFilt")
father1463 <- read_and_normalize("NA12889_Homo_sapiens_Father2_M_1.fastq.dat_Header.txt.rawlengths.sortedFilt")
daughter1463 <- read_and_normalize("NA12878_Homo_sapiens_Daughter2_F_1.fastq.dat_Header.txt.rawlengths.sortedFilt")

#LOAD REPEAT COUNTS FOR ALL DATASETS
filenames<-c("mother","father","son","mother2","father2","daughter2")
read_counts<-c(50131591,40459116,50312390,72528212,75062767,59671707)

files<-list(motherAshkenazim,fatherAshkenazim,sonAshkenazim,mother1463,father1463,daughter1463)

dt_list <- lapply(files, function(x) {
  out <- as.data.table(x)
  setkey(out, "unit")
  out
})

#MERGE ALL DATASETS INTO SINGLE TABLE
mydt <- function(...) merge(..., by="unit", all=T,suffixes=filenames)
data <- Reduce(mydt, dt_list)

data[is.na(data)] <- 0
colnames(data)<-c("unit",filenames)
data$ulength<-nchar(as.character(data$unit))
data<-as.data.frame(data) #convert to dataframe from data table

normalization_factor=1000000
normalization_matrix<-rep.row(read_counts,nrow(data))
data<-as.data.frame(data) #convert to dataframe from data table
data[,2:(ncol(data)-1)]<-(data[,2:(ncol(data)-1)]/normalization_matrix)*normalization_factor #normalization of data by read counts
#data[,2:(ncol(data)-1)]<-log(data[,2:(ncol(data)-1)]+0.01)
#data<-data[rowSums(data[,2:(ncol(data)-1)])>(log(600)),] #keep only rows where all six libraries have at least 300 repeats total

data<-data[order(rowSums(data[,2:(ncol(data)-1)]),decreasing=T),]
#data<-data[order(data$son,decreasing=T),]
#data<-head(data,n=1000) #set more meaningful threshold based on replicates

subset<-head(data,n=1000)

barplot(apply(subset[,2:(ncol(data)-1)],2,sum),col=rainbow(3),ylab="sum of repeat densities",main=paste("Total repeat content for ",nrow(subset),"repeats"))

cor(subset[,2:(ncol(data)-1)])
source("http://www.sthda.com/upload/rquery_cormat.r")
rquery.cormat(subset[,2:(ncol(data)-1)], type="upper")

dist<-as.dist(1-cor((head(data[,2:(ncol(data)-1)],n=1000)),method="spearman")) #distance based on correlation
hcSingle <- hclust(dist,method="ward.D2") 
dendSingle <- as.dendrogram(hcSingle)
#plot(dendSingle, main = "Clustering using linkage function 'complete'",cex=0.3)


#plot(subset[,2:(ncol(subset)-1)])

#from wide to long format
library(tidyr)
library(ggplot2)
library(Rmisc) 
library(scales)
data_long <- gather(subset, member, count, mother:daughter2, factor_key=TRUE)

ggplot(data_long, aes(rep(1:(nrow(data_long)/length(filenames)),times=length(filenames)),count, col=member)) + 
  xlab("index") + 
  scale_y_continuous(labels = comma) +
  labs(title=paste("Using ",(nrow(data_long)/length(filenames)),"repeat motifs"),y="repeat motif density") + 
  theme_bw() + 
  stat_smooth() 

tgc <- summarySE(data_long, measurevar="count", groupvars=c("ulength","member"))
#
ggplot(tgc, aes(x=ulength, y=count, colour=member)) +
    geom_errorbar(aes(ymin=count-se, ymax=count+se), width=.1) +
    geom_line() +
    geom_point() 

#  
@

<<>>=
## Gives count, mean, standard deviation, standard error of the mean, and confidence interval (default 95%).
##   data: a data frame.
##   measurevar: the name of a column that contains the variable to be summariezed
##   groupvars: a vector containing names of columns that contain grouping variables
##   na.rm: a boolean that indicates whether to ignore NA's
##   conf.interval: the percent range of the confidence interval (default is 95%)
summarySE <- function(data=NULL, measurevar, groupvars=NULL, na.rm=FALSE,
                      conf.interval=.95, .drop=TRUE) {
    library(plyr)

    # New version of length which can handle NA's: if na.rm==T, don't count them
    length2 <- function (x, na.rm=FALSE) {
        if (na.rm) sum(!is.na(x))
        else       length(x)
    }

    # This does the summary. For each group's data frame, return a vector with
    # N, mean, and sd
    datac <- ddply(data, groupvars, .drop=.drop,
      .fun = function(xx, col) {
        c(N    = length2(xx[[col]], na.rm=na.rm),
          mean = mean   (xx[[col]], na.rm=na.rm),
          sd   = sd     (xx[[col]], na.rm=na.rm)
        )
      },
      measurevar
    )

    # Rename the "mean" column    
    datac <- rename(datac, c("mean" = measurevar))

    datac$se <- datac$sd / sqrt(datac$N)  # Calculate standard error of the mean

    # Confidence interval multiplier for standard error
    # Calculate t-statistic for confidence interval: 
    # e.g., if conf.interval is .95, use .975 (above/below), and use df=N-1
    ciMult <- qt(conf.interval/2 + .5, datac$N-1)
    datac$ci <- datac$se * ciMult

    return(datac)
}
@

\end{document}